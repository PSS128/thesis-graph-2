Interactive use case: Causal Thesis Builder (user-driven DAG with an LLM co-pilot) Goal: Let the user prove or disprove a thesis by drawing a causal graph (DAG). The user adds/links nodes; the LLM reacts to those specific actions with critiques, counterexamples, confounders, sources, and mini-drafts. Finally, the user can select any subgraph to generate a polished section or a full essay. What the user does Start with a thesis node (e.g., “A 4-day workweek increases productivity”). Add variables (e.g., “meeting hours,” “burnout,” “output per hour”). Draw edges to claim causality (drag from A → B). Merge/rename nodes, group into clusters, pin citations. Lasso-select a subgraph to generate prose for that part. Everything is hands-on: the user is literally constructing the argument. Where the LLM helps (only when the user acts) 1) On text highlight → node User highlights a sentence in a PDF/web page → clicks “Make Node.” LLM returns a canonical variable name + short definition + synonyms. If similar node exists, it suggests merge vs keep separate (with similarity score). Prompt sketch (extraction) You are mapping a causal variable from a highlighted sentence. Return JSON: {name, definition, synonyms[], measurement_ideas[]}. Keep names concise and domain-neutral. 2) On edge draw (A → B) When the user connects two nodes: LLM returns a reason card: plausible mechanisms, directionality checks, and assumption list. Suggests likely confounders using known patterns (e.g., “firm size” confounds “4-day week → productivity”). Prompt sketch (edge rationale) User proposes causality A -> B. Return JSON: {mechanisms[], assumptions[], likely_confounders[], prior_evidence_types[]}. 3) On edge confirm After the user accepts the edge: LLM triggers RAG to fetch candidate citations (quotes + spans) from the user’s corpus and vetted sources. Offers counterexamples (CONTRADICTS edges) if high-quality conflicting evidence appears. Prompt sketch (RAG formatter) Given A->B and its mechanisms, pick 2-3 best passages from retrieved docs. Return JSON: [{title, url, quote, span, supports|contradicts, strength(0..1)}]. 4) On node menu → “Find missing pieces” User asks to strengthen a node: LLM suggests mediators (A → M → B), moderators (“effect stronger when X is high”), and operationalizations (how to measure). It can also propose experiments/observational tests (A/B, IV, diff-in-diff) based on the graph context. Prompt sketch (design ideas) Given the current subgraph around B, suggest mediators, moderators, and test designs. Return JSON: {mediators[], moderators[], study_designs[]}. 5) On graph critique (user toggles a mode) LLM runs quick checks: cycles, over-claiming edges without evidence, “bad controls” warnings (conditioning on colliders), missing back-door adjustments. Flags appear inline with actionable fixes. Prompt sketch (DAG critique) Given nodes/edges with types and citations, detect: - confounding not adjusted - collider/mediator misuse - edges with no evidence Return JSON: {warnings:[{node_or_edge_id, label, fix_suggestion}]}. 6) On lasso-select → “Compose section” LLM converts the selected subgraph into a mini-essay section: claim → mechanisms → evidence → counterarguments → takeaway. Style knobs: academic/op-ed/exec brief; include/exclude pinned citations; length slider. Prompt sketch (subgraph → prose) Compose a cohesive section from this subgraph. Use only ACCEPTED nodes/edges and PINNED citations. Structure: claim, mechanism, evidence (with quotes), counter & rebuttal, takeaway. Tone: {style}. Word budget: {N}. Concrete scenario (to visualize flow) Thesis: “A 4-day workweek increases productivity.” User adds nodes: Work hours, Burnout, Output per hour, Async communication. Draws edges: 4-day week → Burnout (↓), Burnout (↓) → Output per hour (↑). On edge confirm, LLM: Lists mechanisms (“recovery time”, “focus blocks”), assumptions (“no increased meeting density”), and confounders (“firm culture,” “industry”). Fetches 2 supportive studies + 1 mixed result; suggests a CONTRADICTS edge from a meta-analysis with nuance. User lasso-selects the Burnout cluster → Compose section → gets a crisp 300-word subsection with citations. Graph critique warns: “You’re controlling for hours worked in a regression that already mediates the effect—risk of over-controlling.” Suggests fix and highlights alternative model specs. Minimal data schema (abridged) // Node { "id": "n42", "kind": "VARIABLE",               // THESIS | VARIABLE | ASSUMPTION "name": "Burnout", "definition": "Psychological exhaustion reducing work capacity", "synonyms": ["exhaustion", "fatigue"], "citations": [{"doc":"d7","span":[1023,1101]}] } // Edge { "id": "e9", "from": "n4", "to": "n42", "type": "CAUSES",                  // CAUSES | MODERATES | MEDIATES | CONTRADICTS "status": "PROPOSED",              // PROPOSED | ACCEPTED | REJECTED "mechanisms": ["recovery time"], "assumptions": ["meeting load constant"], "confounders": ["firm culture","industry"], "citations": [{"doc":"d12","span":[220,300],"support":"supports","strength":0.73}] } Frontend hooks (how it feels) onHighlightMakeNode(text, sourceRef) → POST /node/extract → add node card under cursor. onCreateEdge(a,b) → POST /edge/rationale → show reason card → user Accept/Reject. onConfirmEdge(edgeId) → POST /edge/evidence (RAG) → side panel shows quotes (pin to attach). onNodeAction(nodeId,“fill_gaps”) → POST /graph/suggest_mediators → ghost nodes/edges appear to accept. onCritique() → POST /graph/critique → inline badges with fixes. onCompose(selection) → POST /compose/subgraph → inject prose into the document pane. Why this nails “interactive with the graph” The user draws, merges, pins, and selects. The LLM is a contextual tool: it reacts to your graph edits with just-in-time rationale, evidence, and fixes. The essay is a byproduct of your curated structure, not a chat transcript. Extras (optional but spicy) What-if toggle: hide an edge or collapse a source; watch the acceptance/confidence heat change; regenerate the affected section. Study design cards: when a path A→…→B exists, suggest feasible empirical strategies (IV, DiD, RCT sketch) with assumptions checklist. Skill mode: “Teach me DAGs here” overlays back-door/front-door tutorials on your actual graph. If you want, I can draft the exact FastAPI endpoints + React Flow handlers (copy-paste ready) and one or two prompt templates you can plug in to see this working on a small sample corpus.



  Immediate Action Items (This Week):

● Start with these 3 tasks:

  1. Review existing backend models - Check backend/app/models/store.py and update Node/Edge schemas
  2. Build the node extraction endpoint - This is the entry point for user interaction
  3. Update the frontend - Add text-to-node and edge-drawing capabilities to GraphCanvas.tsx


#python -m uvicorn app.main:app --reload --port 8000 --log-level debug
